import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel
import re
import sqlvalidator  # pip install sqlvalidator

# =============================
# –°—Ö–µ–º–∞ –ë–î (–ø–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
# =============================
DATABASE_SCHEMA = """
### Database Schema:

Table: Product
Purpose: Contains information about manufacturers and product models
Columns:
  - maker (varchar(10)): Manufacturer name
  - model (varchar(50)): Unique model number (primary key)
  - type (varchar(50)): Product type - 'PC', 'Laptop', or 'Printer'

Table: PC
Purpose: Stores characteristics of desktop computers
Columns:
  - code (int): Unique identifier for each PC
  - model (varchar(50)): Foreign key referencing Product.model
  - speed (smallint): Processor speed in MHz
  - ram (smallint): RAM size in MB
  - hd (real): Hard disk size in GB
  - cd (varchar(10)): CD-ROM speed (e.g., '4x')
  - price (money): Price in dollars

Table: Laptop
Purpose: Stores characteristics of laptops
Columns:
  - code (int): Unique identifier for each laptop
  - model (varchar(50)): Foreign key referencing Product.model
  - speed (smallint): Processor speed in MHz
  - ram (smallint): RAM size in MB
  - hd (real): Hard disk size in GB
  - price (money): Price in dollars
  - screen (tinyint): Screen size in inches

Table: Printer
Purpose: Stores characteristics of printers
Columns:
  - code (int): Unique identifier for each printer
  - model (varchar(50)): Foreign key referencing Product.model
  - color (char(1)): Color capability - 'y' for color, 'n' for monochrome
  - type (varchar(10)): Printer type - 'Laser', 'Jet', or 'Matrix'
  - price (money): Price in dollars

Relationships:
  - Product.model is referenced by PC.model, Laptop.model, and Printer.model
  - Each product type has its own specific table with detailed characteristics
"""

# =============================
# –§–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞
# =============================
PROMPT_TEMPLATE = """### Instruction:
{instruction}

### Input:
{schema}

### Response:
"""

# =============================
# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
# =============================
def load_model_and_tokenizer(model_path: str, adapter_path: str):
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    base_model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    # –ó–∞–≥—Ä—É–∑–∫–∞ LoRA-–∞–¥–∞–ø—Ç–µ—Ä–æ–≤
    model = PeftModel.from_pretrained(base_model, adapter_path)
    model.eval()

    print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    return model, tokenizer

# =============================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL
# =============================
def generate_sql(model, tokenizer, question: str, max_new_tokens: int = 512) -> str:
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
    prompt = PROMPT_TEMPLATE.format(
        instruction=question.strip(),
        schema=DATABASE_SCHEMA.strip()
    )

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    device = "cuda" if torch.cuda.is_available() else "cpu"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(device)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.0,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
    generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
    decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    # –û—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ SQL
    decoded = decoded.strip()
    if "###" in decoded:
        decoded = decoded.split("###")[0].strip()
    if "\n\n" in decoded:
        decoded = decoded.split("\n\n")[0].strip()
    if ";" not in decoded:
        decoded = decoded.rstrip() + ";"
    else:
        decoded = decoded.split(";")[0].strip() + ";"

    return decoded

# =============================
# –ü—Ä–æ–≤–µ—Ä–∫–∞ SQL (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
# =============================
def validate_sql(sql: str) -> bool:
    try:
        sql_query = sqlvalidator.parse(sql)
        return sql_query.is_valid()
    except Exception:
        return False

# =============================
# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ü–∏–∫–ª
# =============================
def main():
    import argparse

    parser = argparse.ArgumentParser(description="Interactive SQL Generation with Fine-tuned Model")
    parser.add_argument("--model_path", type=str, required=True, help="–ü—É—Ç—å –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ./sqlcoder-7b)")
    parser.add_argument("--adapter_path", type=str, required=True, help="–ü—É—Ç—å –∫ –∞–¥–∞–ø—Ç–µ—Ä–∞–º LoRA (–Ω–∞–ø—Ä–∏–º–µ—Ä, ./sqlcoder-finetuned/final_lora)")
    args = parser.parse_args()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model, tokenizer = load_model_and_tokenizer(args.model_path, args.adapter_path)

    print("\n" + "="*80)
    print("üöÄ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ò–ù–§–ï–†–ï–ù–° –ó–ê–ü–£–©–ï–ù")
    print("="*80)
    print("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ.")
    print("–î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ: quit –∏–ª–∏ exit")
    print("="*80 + "\n")

    while True:
        try:
            user_input = input("–í–∞—à –∑–∞–ø—Ä–æ—Å: ").strip()
            if user_input.lower() in ["quit", "exit"]:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            if not user_input:
                continue

            print("üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL...")
            sql = generate_sql(model, tokenizer, user_input)

            print(f"\n‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SQL:\n{sql}\n")

            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞
            if validate_sql(sql):
                print("üü¢ SQL-–∑–∞–ø—Ä–æ—Å —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω.\n")
            else:
                print("üî¥ SQL-–∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏.\n")

            print("-" * 80)

        except KeyboardInterrupt:
            print("\n\nüëã –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}\n")


if __name__ == "__main__":
    main()