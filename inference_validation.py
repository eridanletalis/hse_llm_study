import os
import json
import argparse
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel
import torch
import sqlvalidator  # pip install sqlvalidator

# =============================
# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ)
# =============================
def load_and_split_dataset(file_path: str, val_split: float = 0.1):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏ –¥–µ–ª–∏—Ç –µ–≥–æ –Ω–∞ train/val —Å —Ç–µ–º –∂–µ random_state=42"""
    if file_path.endswith('.jsonl'):
        with open(file_path, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
    elif file_path.endswith('.json'):
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    elif file_path.endswith('.csv'):
        df = pd.read_csv(file_path)
        data = df.to_dict('records')
    else:
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ .json, .jsonl, .csv")

    for item in data:
        if "instruction" not in item or "output" not in item:
            raise ValueError("–ö–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'instruction' –∏ 'output'")
        if "input" not in item:
            item["input"] = ""

    # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–æ–º–ø—Ç (–∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏)
    def format_instruction(example):
        if example['input']:
            return f"""### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
"""
        else:
            return f"""### Instruction:
{example['instruction']}

### Response:
"""

    texts = [format_instruction(item) for item in data]
    dataset = Dataset.from_dict({"text": texts, "output": [item["output"] for item in data]})

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å —Ç–µ–º –∂–µ random_state=42
    dataset = dataset.train_test_split(test_size=val_split, seed=42)
    return dataset["test"]  # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É


# =============================
# –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL
# =============================
def generate_sql(model, tokenizer, prompt: str, max_new_tokens: int = 512) -> str:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.0,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
    generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
    decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    # –û—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã, –æ–±—Ä–µ–∑–∞–µ–º –ø–æ —Ç–æ—á–∫–µ —Å –∑–∞–ø—è—Ç–æ–π
    decoded = decoded.strip()
    if "###" in decoded:
        decoded = decoded.split("###")[0].strip()
    if "\n\n" in decoded:
        decoded = decoded.split("\n\n")[0].strip()
    if ";" in decoded:
        decoded = decoded.split(";")[0].strip() + ";"
    else:
        decoded = decoded.rstrip() + ";"

    return decoded


# =============================
# –ü—Ä–æ–≤–µ—Ä–∫–∞ SQL –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å
# =============================
def is_sql_valid(sql: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ SQL-–∑–∞–ø—Ä–æ—Å —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º"""
    try:
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
        sql = sql.strip().rstrip(";") + ";"
        sql_query = sqlvalidator.parse(sql)
        return sql_query.is_valid()
    except Exception:
        return False


# =============================
# –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç
# =============================
def main():
    parser = argparse.ArgumentParser(description="Inference on validation set")
    parser.add_argument("--model_path", type=str, required=True, help="–ü—É—Ç—å –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--adapter_path", type=str, required=True, help="–ü—É—Ç—å –∫ –∞–¥–∞–ø—Ç–µ—Ä–∞–º LoRA")
    parser.add_argument("--data_path", type=str, required=True, help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏")
    parser.add_argument("--val_split", type=float, default=0.1, help="–î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏")
    parser.add_argument("--max_new_tokens", type=int, default=512, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤")
    parser.add_argument("--output_file", type=str, default="inference_results.jsonl", help="–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    args = parser.parse_args()

    print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏...")
    val_dataset = load_and_split_dataset(args.data_path, args.val_split)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(val_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")

    # =============================
    # –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    # =============================
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    base_model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    base_model.eval()

    # =============================
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (—Å –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏)
    # =============================
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏...")
    fine_tuned_model = PeftModel.from_pretrained(base_model, args.adapter_path)
    fine_tuned_model.eval()

    # =============================
    # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∏ –æ—Ü–µ–Ω–∫–∞
    # =============================
    results = []
    base_correct = 0
    ft_correct = 0

    print("üöÄ –ù–∞—á–∞–ª–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞...")
    for i, example in enumerate(val_dataset):
        prompt = example["text"]
        true_sql = example["output"]

        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        base_sql = generate_sql(base_model, tokenizer, prompt, args.max_new_tokens)

        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        ft_sql = generate_sql(fine_tuned_model, tokenizer, prompt, args.max_new_tokens)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏
        base_valid = is_sql_valid(base_sql)
        ft_valid = is_sql_valid(ft_sql)

        if base_valid:
            base_correct += 1
        if ft_valid:
            ft_correct += 1

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "id": i + 1,
            "prompt": prompt,
            "true_sql": true_sql,
            "base_model_sql": base_sql,
            "base_model_valid": base_valid,
            "fine_tuned_sql": ft_sql,
            "fine_tuned_valid": ft_valid
        }
        results.append(result)

        if (i + 1) % 10 == 0:
            print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1} / {len(val_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # =============================
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    # =============================
    with open(args.output_file, 'w', encoding='utf-8') as f:
        for item in results:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    # =============================
    # –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
    # =============================
    total = len(val_dataset)
    base_accuracy = base_correct / total
    ft_accuracy = ft_correct / total

    print("\n" + "="*60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò–ù–§–ï–†–ï–ù–°–ê")
    print("="*60)
    print(f"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {total}")
    print(f"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å ‚Äî –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö SQL: {base_correct} ({base_accuracy:.2%})")
    print(f"–î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å ‚Äî –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö SQL: {ft_correct} ({ft_accuracy:.2%})")
    print(f"–£–ª—É—á—à–µ–Ω–∏–µ: {ft_accuracy - base_accuracy:+.2%}")
    print("="*60)

    if ft_accuracy > base_accuracy:
        print("üéâ –î–æ–æ–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–∏–ª–æ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL!")
    elif ft_accuracy == base_accuracy:
        print("‚û°Ô∏è –ö–∞—á–µ—Å—Ç–≤–æ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–∞ —Ç–æ–º –∂–µ —É—Ä–æ–≤–Ω–µ.")
    else:
        print("‚ö†Ô∏è –ö–∞—á–µ—Å—Ç–≤–æ —É—Ö—É–¥—à–∏–ª–æ—Å—å ‚Äî –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è.")

    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {args.output_file}")


if __name__ == "__main__":
    main()