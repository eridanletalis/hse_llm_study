import os
import json
import logging
import argparse
import pandas as pd
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
import torch
import psutil
import GPUtil
import matplotlib.pyplot as plt

# =============================
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
# =============================
def setup_logging(log_dir: str = "./logs"):
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, "training.log")

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


# =============================
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
# =============================
def load_dataset_from_file(file_path: str) -> Dataset:
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ {file_path}...")
    if file_path.endswith('.jsonl'):
        with open(file_path, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
    elif file_path.endswith('.json'):
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    elif file_path.endswith('.csv'):
        df = pd.read_csv(file_path)
        data = df.to_dict('records')
    else:
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ .json, .jsonl, .csv")

    for item in data:
        if "instruction" not in item or "output" not in item:
            raise ValueError("–ö–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'instruction' –∏ 'output'")
        if "input" not in item:
            item["input"] = ""

    def format_instruction(example):
        if example['input']:
            return f"""### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
{example['output']}"""
        else:
            return f"""### Instruction:
{example['instruction']}

### Response:
{example['output']}"""

    texts = [format_instruction(item) for item in data]
    logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} –ø—Ä–∏–º–µ—Ä–æ–≤.")
    return Dataset.from_dict({"text": texts})


# =============================
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU
# =============================
def log_gpu_memory():
    gpus = GPUtil.getGPUs()
    for gpu in gpus:
        logger.info(f"GPU {gpu.id}: {gpu.memoryUsed}MB / {gpu.memoryTotal}MB (Free: {gpu.memoryFree}MB)")


# =============================
# –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç
# =============================
def main():
    parser = argparse.ArgumentParser(description="Fine-tune LLM with QLoRA")
    parser.add_argument("--model_path", type=str, required=True, help="–ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--data_path", type=str, required=True, help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    parser.add_argument("--output_dir", type=str, default="./results", help="–ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    parser.add_argument("--epochs", type=int, default=3, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size (–Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ)")
    parser.add_argument("--grad_accum_steps", type=int, default=4, help="–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è")
    parser.add_argument("--learning_rate", type=float, default=2e-4, help="Learning rate")
    parser.add_argument("--max_seq_length", type=int, default=1024, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    parser.add_argument("--val_split", type=float, default=0.1, help="–î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ (0.1 = 10%)")
    args = parser.parse_args()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
    global logger
    logger = setup_logging(os.path.join(args.output_dir, "logs"))

    logger.info("="*60)
    logger.info("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
    logger.info("="*60)
    logger.info(f"–ú–æ–¥–µ–ª—å: {args.model_path}")
    logger.info(f"–î–∞–Ω–Ω—ã–µ: {args.data_path}")
    logger.info(f"–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {args.output_dir}")
    logger.info(f"–≠–ø–æ—Ö–∏: {args.epochs}")
    logger.info(f"Batch size: {args.batch_size}")
    logger.info(f"Gradient accumulation steps: {args.grad_accum_steps}")
    logger.info(f"Learning rate: {args.learning_rate}")
    logger.info(f"Max sequence length: {args.max_seq_length}")
    logger.info(f"Validation split: {args.val_split}")

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    logger.info("üìä –ü–∞–º—è—Ç—å –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏:")
    log_gpu_memory()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    full_dataset = load_dataset_from_file(args.data_path)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train / val
    if args.val_split > 0:
        dataset_dict = full_dataset.train_test_split(test_size=args.val_split, seed=42)
        train_dataset = dataset_dict["train"]
        eval_dataset = dataset_dict["test"]
        logger.info(f"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_dataset)} train, {len(eval_dataset)} val")
    else:
        train_dataset = full_dataset
        eval_dataset = None
        logger.info("–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.")

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    logger.info("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è k-bit –æ–±—É—á–µ–Ω–∏—è...")
    model = prepare_model_for_kbit_training(model)

    # LoRA –∫–æ–Ω—Ñ–∏–≥
    peft_config = LoraConfig(
        r=256,  # –±—ã–ª–æ 64 –∏ 128, –ø–æ–º–µ–Ω—è–ª–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º
        lora_alpha=16,
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
    )


    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    logger.info("üìä –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏:")
    log_gpu_memory()

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    logger.info("üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=args.max_seq_length,
            return_tensors="pt"
        )

    tokenized_train = train_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=["text"]
    )

    if eval_dataset:
        tokenized_eval = eval_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )
    else:
        tokenized_eval = None

    # Training Arguments ‚Äî –ë–ï–ó wandb
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.grad_accum_steps,
        optim="paged_adamw_8bit",
        save_strategy="steps",
        eval_strategy="steps" if eval_dataset else "no",
        eval_steps=50 if eval_dataset else None,
        logging_steps=10,
        learning_rate=args.learning_rate,
        fp16=True,
        max_grad_norm=0.3,
        warmup_ratio=0.03,
        lr_scheduler_type="cosine",
        report_to="none",  # –û—Ç–∫–ª—é—á–∞–µ–º wandb –∏ –¥—Ä—É–≥–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        save_total_limit=2,
        load_best_model_at_end=True if eval_dataset else False,
        metric_for_best_model="eval_loss" if eval_dataset else None,
        greater_is_better=False if eval_dataset else None,
        push_to_hub=False,
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ ‚Äî –ë–ï–ó wandb
    logger.info("üèãÔ∏è‚Äç‚ôÇÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞...")
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_eval,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
        peft_config=peft_config
    )

    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
    train_loss_history = []
    eval_loss_history = []
    steps = []

    # Callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
    class LossHistoryCallback(TrainerCallback):
        def on_log(self, args, state, control, logs=None, **kwargs):
            if logs is not None:
                if "loss" in logs:
                    train_loss_history.append(logs["loss"])
                    steps.append(state.global_step)
                if "eval_loss" in logs:
                    eval_loss_history.append(logs["eval_loss"])

    trainer.add_callback(LossHistoryCallback())

    # –û–±—É—á–µ–Ω–∏–µ
    logger.info("üî• –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    train_result = trainer.train()

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è:")
    logger.info(f"  –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤: {train_result.global_step}")
    logger.info(f"  –ü–æ—Å–ª–µ–¥–Ω—è—è train loss: {train_result.training_loss}")

    if eval_dataset:
        eval_result = trainer.evaluate()
        logger.info(f"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è loss: {eval_result['eval_loss']}")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–∞–π–ª
        with open(os.path.join(args.output_dir, "eval_results.json"), "w", encoding="utf-8") as f:
            json.dump(eval_result, f, ensure_ascii=False, indent=2)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
    final_lora_path = os.path.join(args.output_dir, "final_lora")
    model.save_pretrained(final_lora_path)
    tokenizer.save_pretrained(final_lora_path)

    logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    logger.info(f"–ê–¥–∞–ø—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {final_lora_path}")

    logger.info("üìä –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:")
    log_gpu_memory()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –ª–æ—Å—Å–æ–≤
    history = {
        "steps": steps,
        "train_loss": train_loss_history,
        "eval_loss": eval_loss_history
    }
    with open(os.path.join(args.output_dir, "training_history.json"), "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if len(train_loss_history) > 0:
        os.makedirs(os.path.join(args.output_dir, "plots"), exist_ok=True)
        plt.figure(figsize=(10, 6))
        plt.plot(steps, train_loss_history, label="Train Loss", marker='o')
        if len(eval_loss_history) > 0:
            eval_steps = steps[::len(steps)//len(eval_loss_history)][:len(eval_loss_history)]  # –ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ
            plt.plot(eval_steps, eval_loss_history, label="Eval Loss", marker='s')
        plt.xlabel("Steps")
        plt.ylabel("Loss")
        plt.title("Training and Validation Loss")
        plt.legend()
        plt.grid(True)
        plt.savefig(os.path.join(args.output_dir, "plots", "loss_curve.png"))
        plt.close()
        logger.info("üìà –ì—Ä–∞—Ñ–∏–∫ loss —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ plots/loss_curve.png")

    logger.info("="*60)
    logger.info("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    logger.info("="*60)


# –î–æ–±–∞–≤–ª—è–µ–º TrainerCallback –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
from transformers import TrainerCallback

if __name__ == "__main__":
    main()